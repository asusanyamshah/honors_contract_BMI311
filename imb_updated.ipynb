{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2401, 13)\n",
      "1579\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "\n",
    "# Defining the path for the csv file\n",
    "path = os.path.join(\"dataset.csv\")\n",
    "\n",
    "# Storing the dataframe in a variable named dataset\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "dataset = dataset.drop('seqn', axis='columns')\n",
    "dataset = dataset.drop('Marital', axis='columns')\n",
    "print(dataset.shape)\n",
    "print(len(dataset[dataset['MetabolicSyndrome'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train dtypes: float64\n",
      "y_train dtype: int64\n",
      "Accracy: \n",
      "\t86.25% is the accuracy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/p012qq114lj2w2v6rb72_p1c0000gn/T/ipykernel_32256/2607916315.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
      "/var/folders/8t/p012qq114lj2w2v6rb72_p1c0000gn/T/ipykernel_32256/2607916315.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Race'] = dataset['Race'].replace(race_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Accuracy 0.85625\n",
      "KNN Accuracy: 85.62%\n",
      "SMOTE Accuracy: 86.62%\n",
      "SMOTE f1: 0.86\n",
      "SMOTE Recall: 0.82\n",
      "SMOTE Precision: 0.90\n",
      "\n",
      "\n",
      "\n",
      "SMOTETomek Accuracy: 85.62%\n",
      "SMOTETomek f1: 0.85\n",
      "SMOTETomek Recall: 0.80\n",
      "SMOTETomek Precision: 0.90\n",
      "\n",
      "\n",
      "\n",
      "SMOTEENN Accuracy: 86.38%\n",
      "SMOTEENN f1: 0.87\n",
      "SMOTEENN Recall: 0.88\n",
      "SMOTEENN Precision: 0.85\n",
      "\n",
      "\n",
      "\n",
      "ADASYN Accuracy: 86.62%\n",
      "ADASYN f1: 0.86\n",
      "ADASYN Recall: 0.83\n",
      "ADASYN Precision: 0.90\n",
      "\n",
      "\n",
      "\n",
      "CTGAN Accuracy: 55.25%\n",
      "CTGAN f1: 0.37\n",
      "CTGAN Recall: 0.27\n",
      "CTGAN Precision: 0.62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Confusion matrix: \")\\nprint(cm, \"\\n\")\\nprint(\"Precision Score: \")\\nprint(\"\\t\",precision_score(y_test, y_pred), \"\\n\")\\nprint(\"Recall: \")\\nprint(\"\\t\", recall_score(y_test, y_pred), \"\\n\")\\nprint(\"F1 Score: \")\\nprint(\"\\t\", f1_score(y_test, y_pred), \"\\n\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the categorical variables and filling in the missing values\n",
    "sex_mapping = {'Male': 0, 'Female': 1}\n",
    "race_mapping = {'White': 0, 'Asian': 1, 'Black': 2, 'MexAmerican': 3, 'Hispanic': 4, 'Other': 5}\n",
    "\n",
    "dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
    "dataset['Race'] = dataset['Race'].replace(race_mapping)\n",
    "\n",
    "# This is the incorrect implementation\n",
    "'''\n",
    "dataset = dataset.fillna(2)\n",
    "dataset = dataset.fillna(4)\n",
    "dataset = dataset.fillna(5)\n",
    "'''\n",
    "# Fill NaN values in column with index 2\n",
    "dataset.iloc[:, 2] = dataset.iloc[:, 2].fillna(dataset.iloc[:, 2].mean())\n",
    "\n",
    "# Fill NaN values in column with index 4\n",
    "dataset.iloc[:, 4] = dataset.iloc[:, 4].fillna(dataset.iloc[:, 4].mean())\n",
    "\n",
    "# Fill NaN values in column with index 5\n",
    "dataset.iloc[:, 5] = dataset.iloc[:, 5].fillna(dataset.iloc[:, 5].mean())\n",
    "\n",
    "outcome_0 = dataset[dataset['MetabolicSyndrome'] == 0]\n",
    "outcome_1 = dataset[dataset['MetabolicSyndrome'] == 1]\n",
    "\n",
    "\n",
    "test_size_each_class = 400\n",
    "test_0 = outcome_0.sample(n=test_size_each_class, random_state=42)\n",
    "test_1 = outcome_1.sample(n=test_size_each_class, random_state=42)\n",
    "#print(test_1)\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Remove the test set rows from the original dataset to create the training set\n",
    "train_data = dataset.drop(test_data.index)\n",
    "\n",
    "\n",
    "x_train = train_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_train = train_data['MetabolicSyndrome'].values\n",
    "x_test = test_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_test = test_data['MetabolicSyndrome'].values\n",
    "\n",
    "# Resampling the data to avoid overfitting\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "# Resampling the data\n",
    "x_resampled, y_resampled = ros.fit_resample(x_train, y_train)\n",
    "print(\"x_train dtypes:\", x_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "\n",
    "# XGBoost Classifier\n",
    "classifier_xgboost = XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.5)\n",
    "classifier_xgboost = XGBClassifier(n_estimators = 412, max_depth = 8, learning_rate = 0.5)\n",
    "classifier_xgboost.fit(x_resampled, y_resampled)\n",
    "# joblib.dump(classifier_xgboost, 'xgboost_classifier.pkl')\n",
    "y_pred = classifier_xgboost.predict(x_test)\n",
    "#for i in x_test[0]:\n",
    "   # print(type(i))\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accracy: \")\n",
    "print(f'\\t{(accuracy_score(y_test, y_pred) * 100):.2f}% is the accuracy\\n')\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from ctgan import CTGAN\n",
    "\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(x_train, y_train, x_test, y_test, model):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    return accuracy, f1, recall, precision\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "smote_accuracy, smote_f1, smore_recall, smore_precision = evaluate_model(x_smote, y_smote, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3)\n",
    " )\n",
    "smote_xg = XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.3)\n",
    "smote_xg.fit(x_smote, y_smote)\n",
    "\n",
    "smote_t = SMOTETomek(random_state=0)\n",
    "x_smote_t, y_smote_t = smote_t.fit_resample(x_train, y_train)\n",
    "smote_t_accuracy, smote_t_f1, smote_t_recall, smote_t_precision = evaluate_model(x_smote_t, y_smote_t, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "\n",
    "smote_tn = SMOTEENN(random_state=0)\n",
    "x_smote_tn, y_smote_tn = smote_tn.fit_resample(x_train, y_train)\n",
    "smote_tn_accuracy, smote_tn_f1, smote_tn_recall, smote_tn_precision = evaluate_model(x_smote_tn, y_smote_tn, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "smote_tn_model = XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.3)\n",
    "smote_xg.fit(x_smote_tn, y_smote_tn) \n",
    "\n",
    "\n",
    "# ADASYN\n",
    "adasyn = ADASYN(random_state=0)\n",
    "x_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "adasyn_accuracy, adasyn_f1, adasyn_recall, adasyn_precision = evaluate_model(x_adasyn, y_adasyn, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "adasyn_xg = XGBClassifier(n_estimators = 412, max_depth = 8, learning_rate = 0.3)\n",
    "adasyn_xg.fit(x_adasyn, y_adasyn)\n",
    "\n",
    "# CTGAN\n",
    "ctgan = CTGAN(epochs=10)\n",
    "ctgan.fit(x_train, y_train)\n",
    "x_ctgan = ctgan.sample(len(x_train))\n",
    "y_ctgan = y_train  # Optionally reuse y_train if it makes sense for your scenario\n",
    "ctgan_accuracy, ctgan_f1, ctgan_recall, ctgan_precision = evaluate_model(x_ctgan, y_ctgan, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "dataset_imputed = imputer.fit_transform(dataset)\n",
    "dataset_imputed = pd.DataFrame(dataset_imputed, columns=dataset.columns)\n",
    "outcome_0 = dataset_imputed[dataset_imputed['MetabolicSyndrome'] == 0]\n",
    "outcome_1 = dataset_imputed[dataset_imputed['MetabolicSyndrome'] == 1]\n",
    "\n",
    "\n",
    "test_size_each_class = 400\n",
    "test_0 = outcome_0.sample(n=test_size_each_class, random_state=42)\n",
    "test_1 = outcome_1.sample(n=test_size_each_class, random_state=42)\n",
    "#print(test_1)\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Remove the test set rows from the original dataset to create the training set\n",
    "train_data = dataset.drop(test_data.index)\n",
    "\n",
    "\n",
    "x_train = train_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_train = train_data['MetabolicSyndrome'].values\n",
    "x_test = test_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_test = test_data['MetabolicSyndrome'].values\n",
    "\n",
    "# XGBoost Classifier\n",
    "classifier_xgboost = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5)\n",
    "classifier_xgboost.fit(x_train, y_train)\n",
    "y_pred = classifier_xgboost.predict(x_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "mode_merged_saved = joblib.load('merged_model.pkl')\n",
    "y_pred_saved = mode_merged_saved.predict(x_test)\n",
    "acc_saved = accuracy_score(y_test, y_pred)\n",
    "print(\"Saved Accuracy\", acc_saved)\n",
    "\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[('smote', smote_xg), ('adasyn', adasyn_xg)], voting='soft'\n",
    ")\n",
    "print(f'KNN Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'SMOTE Accuracy: {smote_accuracy * 100:.2f}%')\n",
    "print(f'SMOTE f1: {smote_f1:.2f}')\n",
    "print(f'SMOTE Recall: {smore_recall:.2f}')\n",
    "print(f'SMOTE Precision: {smore_precision:.2f}')\n",
    "print(\"\\n\\n\")\n",
    "print(f'SMOTETomek Accuracy: {smote_t_accuracy * 100:.2f}%')\n",
    "print(f'SMOTETomek f1: {smote_t_f1:.2f}')\n",
    "print(f'SMOTETomek Recall: {smote_t_recall:.2f}')\n",
    "print(f'SMOTETomek Precision: {smote_t_precision:.2f}')\n",
    "print(\"\\n\\n\")\n",
    "print(f'SMOTEENN Accuracy: {smote_tn_accuracy * 100:.2f}%')\n",
    "print(f'SMOTEENN f1: {smote_tn_f1:.2f}')\n",
    "print(f'SMOTEENN Recall: {smote_tn_recall:.2f}')\n",
    "print(f'SMOTEENN Precision: {smote_tn_precision:.2f}')\n",
    "print(\"\\n\\n\")\n",
    "print(f'ADASYN Accuracy: {adasyn_accuracy * 100:.2f}%')\n",
    "print(f'ADASYN f1: {adasyn_f1:.2f}')\n",
    "print(f'ADASYN Recall: {adasyn_recall:.2f}')\n",
    "print(f'ADASYN Precision: {adasyn_precision:.2f}')\n",
    "print(\"\\n\\n\")\n",
    "print(f'CTGAN Accuracy: {ctgan_accuracy * 100:.2f}%')\n",
    "print(f'CTGAN f1: {ctgan_f1:.2f}')\n",
    "print(f'CTGAN Recall: {ctgan_recall:.2f}')\n",
    "print(f'CTGAN Precision: {ctgan_precision:.2f}')\n",
    "'''\n",
    "print(\"Confusion matrix: \")\n",
    "print(cm, \"\\n\")\n",
    "print(\"Precision Score: \")\n",
    "print(\"\\t\",precision_score(y_test, y_pred), \"\\n\")\n",
    "print(\"Recall: \")\n",
    "print(\"\\t\", recall_score(y_test, y_pred), \"\\n\")\n",
    "print(\"F1 Score: \")\n",
    "print(\"\\t\", f1_score(y_test, y_pred), \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution - Majority: 1179, Minority: 422\n",
      "Number of synthetic samples - SMOTE: 757, ADASYN: 709\n",
      "Best weights found: [0.8, 0.19999999999999996]\n",
      "\n",
      "Evaluating individual methods...\n",
      "\n",
      "SMOTE Results:\n",
      "Accuracy: 86.62%\n",
      "F1-Score: 0.86\n",
      "Recall: 0.82\n",
      "Precision: 0.90\n",
      "\n",
      "ADASYN Results:\n",
      "Accuracy: 87.12%\n",
      "F1-Score: 0.87\n",
      "Recall: 0.84\n",
      "Precision: 0.90\n",
      "\n",
      "SMOTE-Tomek Results:\n",
      "Accuracy: 86.38%\n",
      "F1-Score: 0.86\n",
      "Recall: 0.83\n",
      "Precision: 0.89\n",
      "\n",
      "Merged Approach Results:\n",
      "Accuracy: 85.50%\n",
      "F1-Score: 0.85\n",
      "Recall: 0.81\n",
      "Precision: 0.89\n",
      "\n",
      "Ensemble Results:\n",
      "Accuracy: 86.88%\n",
      "F1-Score: 0.86\n",
      "Recall: 0.84\n",
      "Precision: 0.89\n",
      "\n",
      "Best performing method: ADASYN (F1-Score: 0.87)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def improved_merge_synthetic_samples(synthetic_samples_list, weights=None):\n",
    "    \"\"\"\n",
    "    Improved version of merge_synthetic_samples with diversity preservation\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1/len(synthetic_samples_list)] * len(synthetic_samples_list)\n",
    "    \n",
    "    # Ensure weights sum to 1\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    \n",
    "    # Calculate number of samples to take from each method\n",
    "    total_samples = len(synthetic_samples_list[0])\n",
    "    samples_per_method = [int(w * total_samples) for w in weights]\n",
    "    \n",
    "    # Adjust the last value to ensure we get exactly total_samples\n",
    "    samples_per_method[-1] = total_samples - sum(samples_per_method[:-1])\n",
    "    \n",
    "    # Initialize merged samples array\n",
    "    merged_samples = []\n",
    "    \n",
    "    # Take stratified samples from each method\n",
    "    for samples, n_samples in zip(synthetic_samples_list, samples_per_method):\n",
    "        # Randomly select samples without replacement\n",
    "        indices = np.random.choice(len(samples), n_samples, replace=False)\n",
    "        merged_samples.append(samples[indices])\n",
    "    \n",
    "    # Concatenate all samples\n",
    "    merged_samples = np.vstack(merged_samples)\n",
    "    \n",
    "    # Shuffle the merged samples\n",
    "    np.random.shuffle(merged_samples)\n",
    "    \n",
    "    return merged_samples\n",
    "\n",
    "def evaluate_model(x_train, y_train, x_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using multiple metrics\n",
    "    \"\"\"\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, f1, recall, precision\n",
    "\n",
    "def find_best_weights(synthetic_samples_list, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Find the best weights for merging synthetic samples using cross-validation\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    # Try different weight combinations\n",
    "    for w1 in np.arange(0.1, 1.0, 0.1):\n",
    "        w2 = 1 - w1\n",
    "        weights = [w1, w2]\n",
    "        \n",
    "        merged_synthetic = improved_merge_synthetic_samples(synthetic_samples_list, weights)\n",
    "        x_merged = np.vstack([x_train, merged_synthetic])\n",
    "        y_merged = np.concatenate([y_train, np.ones(len(merged_synthetic))])\n",
    "        \n",
    "        model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.25,\n",
    "                            reg_lambda=1.0, reg_alpha=0.1)\n",
    "        \n",
    "        # Use cross-validation to evaluate this weight combination\n",
    "        cv_scores = cross_val_score(model, x_merged, y_merged, cv=5, scoring='f1')\n",
    "        avg_f1 = np.mean(cv_scores)\n",
    "        \n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            best_weights = weights\n",
    "    \n",
    "    return best_weights\n",
    "\n",
    "def main(x_train, y_train, x_test, y_test):\n",
    "    # Print original class distribution\n",
    "    n_minority = np.sum(y_train == 1)\n",
    "    n_majority = np.sum(y_train == 0)\n",
    "    print(f\"Original distribution - Majority: {n_majority}, Minority: {n_minority}\")\n",
    "    \n",
    "    # Generate synthetic samples with SMOTE and ADASYN\n",
    "    smote = SMOTE(random_state=42)\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "    x_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "    x_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # Extract only the synthetic samples\n",
    "    synthetic_smote = x_smote[len(x_train):]\n",
    "    synthetic_adasyn = x_adasyn[len(x_train):]\n",
    "    \n",
    "    print(f\"Number of synthetic samples - SMOTE: {len(synthetic_smote)}, ADASYN: {len(synthetic_adasyn)}\")\n",
    "    \n",
    "    if len(synthetic_smote) > 0 and len(synthetic_adasyn) > 0:\n",
    "        # Find best weights for merging\n",
    "        synthetic_samples_list = [synthetic_smote, synthetic_adasyn]\n",
    "        best_weights = find_best_weights(synthetic_samples_list, x_train, y_train, x_test, y_test)\n",
    "        print(f\"Best weights found: {best_weights}\")\n",
    "        \n",
    "        # Merge synthetic samples with best weights\n",
    "        merged_synthetic = improved_merge_synthetic_samples(synthetic_samples_list, best_weights)\n",
    "        \n",
    "        # Prepare datasets\n",
    "        x_merged = np.vstack([x_train, merged_synthetic])\n",
    "        y_merged = np.concatenate([y_train, np.ones(len(merged_synthetic))])\n",
    "        \n",
    "        # Initialize models with regularization\n",
    "        base_model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.25)\n",
    "        merged_model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.25,\n",
    "                                   reg_lambda=1.0, reg_alpha=0.1)\n",
    "        ensemble_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('smote', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3)),\n",
    "                ('adasyn', RandomForestClassifier(n_estimators=100)),\n",
    "                ('merged', merged_model),\n",
    "                ('smote_tomek', XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        print(\"\\nEvaluating individual methods...\")\n",
    "        smote_metrics = evaluate_model(x_smote, y_smote, x_test, y_test, base_model)\n",
    "        adasyn_metrics = evaluate_model(x_adasyn, y_adasyn, x_test, y_test, base_model)\n",
    "        smote_tomek_metrics = evaluate_model(x_smote_tomek, y_smote_tomek, x_test, y_test, base_model)\n",
    "        merged_metrics = evaluate_model(x_merged, y_merged, x_test, y_test, merged_model)\n",
    "        ensemble_metrics = evaluate_model(x_merged, y_merged, x_test, y_test, ensemble_model)\n",
    "        \n",
    "        # Print results\n",
    "        methods = ['SMOTE', 'ADASYN', 'SMOTE-Tomek', 'Merged Approach', 'Ensemble']\n",
    "        metrics = [smote_metrics, adasyn_metrics, smote_tomek_metrics, merged_metrics, ensemble_metrics]\n",
    "        \n",
    "        for method, (acc, f1, rec, prec) in zip(methods, metrics):\n",
    "            print(f\"\\n{method} Results:\")\n",
    "            print(f\"Accuracy: {acc * 100:.2f}%\")\n",
    "            print(f\"F1-Score: {f1:.2f}\")\n",
    "            print(f\"Recall: {rec:.2f}\")\n",
    "            print(f\"Precision: {prec:.2f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        best_f1_score = max(m[1] for m in metrics)\n",
    "        best_method_idx = [m[1] for m in metrics].index(best_f1_score)\n",
    "        best_method = methods[best_method_idx]\n",
    "        \n",
    "        print(f\"\\nBest performing method: {best_method} (F1-Score: {best_f1_score:.2f})\")\n",
    "        \n",
    "        if best_method == 'Merged Approach':\n",
    "            joblib.dump(merged_model, 'best_model.pkl')\n",
    "        elif best_method == 'Ensemble':\n",
    "            joblib.dump(ensemble_model, 'best_model.pkl')\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: One or more synthetic sample sets are empty\")\n",
    "        print(\"Please ensure the minority class has samples to generate synthetic data\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your data split into x_train, y_train, x_test, y_test\n",
    "main(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution - Majority: 1179, Minority: 422\n",
      "Number of synthetic samples - SMOTE: 757, ADASYN: 709\n",
      "SMOTE Accuracy: 86.38%\n",
      "SMOTE f1: 0.86\n",
      "SMOTE Recall: 0.82\n",
      "SMOTE Precision: 0.89\n",
      "\n",
      "\n",
      "\n",
      "ADASYN Accuracy: 86.38%\n",
      "ADASYN f1: 0.86\n",
      "ADASYN Recall: 0.82\n",
      "ADASYN Precision: 0.90\n",
      "\n",
      "\n",
      "\n",
      "Merged Approach Accuracy: 86.75%\n",
      "Merged Approach f1: 0.86\n",
      "Merged Approach Recall: 0.83\n",
      "Merged Approach Precision: 0.90\n",
      "\n",
      "\n",
      "\n",
      "Ensemble approach accuracy: 87.00\n",
      "Enseble F1: 0.8649350649350651\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def merge_synthetic_samples(synthetic_samples_list, weights=None):\n",
    "    \"\"\"\n",
    "    Merge multiple sets of synthetic samples using weighted nearest neighbor averaging.\n",
    "    \n",
    "    Parameters:\n",
    "    synthetic_samples_list: List of numpy arrays, each containing synthetic samples from different methods\n",
    "    weights: List of weights for each method (default: equal weights)\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: Merged synthetic samples\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1/len(synthetic_samples_list)] * len(synthetic_samples_list)\n",
    "    \n",
    "    # Ensure we have valid samples\n",
    "    for i, samples in enumerate(synthetic_samples_list):\n",
    "        if len(samples) == 0:\n",
    "            raise ValueError(f\"Synthetic samples list at index {i} is empty\")\n",
    "    \n",
    "    # Ensure all sample sets have the same number of features\n",
    "    n_features = synthetic_samples_list[0].shape[1]\n",
    "    \n",
    "    # Initialize nearest neighbor models for each synthetic dataset\n",
    "    nn_models = []\n",
    "    for samples in synthetic_samples_list:\n",
    "        nn = NearestNeighbors(n_neighbors=1)\n",
    "        nn.fit(samples)\n",
    "        nn_models.append(nn)\n",
    "    \n",
    "    # Use the first synthetic dataset as a reference\n",
    "    base_samples = synthetic_samples_list[0]\n",
    "    merged_samples = np.zeros_like(base_samples)\n",
    "    \n",
    "    # For each sample in the base dataset\n",
    "    for i in range(len(base_samples)):\n",
    "        sample_sum = np.zeros(n_features)\n",
    "        \n",
    "        # Find nearest neighbors in each synthetic dataset\n",
    "        for j, (samples, nn_model, weight) in enumerate(zip(synthetic_samples_list, nn_models, weights)):\n",
    "            if j == 0:\n",
    "                # For the base dataset, use the sample itself\n",
    "                nearest_sample = base_samples[i]\n",
    "            else:\n",
    "                # Find nearest neighbor in other datasets\n",
    "                distances, indices = nn_model.kneighbors([base_samples[i]])\n",
    "                nearest_sample = samples[indices[0][0]]\n",
    "            \n",
    "            sample_sum += weight * nearest_sample\n",
    "        \n",
    "        merged_samples[i] = sample_sum\n",
    "    \n",
    "    return merged_samples\n",
    "\n",
    "# First, let's analyze the class distribution\n",
    "n_minority = np.sum(y_train == 1)\n",
    "n_majority = np.sum(y_train == 0)\n",
    "print(f\"Original distribution - Majority: {n_majority}, Minority: {n_minority}\")\n",
    "\n",
    "# Generate synthetic samples with SMOTE and ADASYN\n",
    "smote = SMOTE(random_state=42)\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Generate synthetic samples\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "x_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "\n",
    "# Extract only the synthetic samples (exclude original samples)\n",
    "synthetic_smote = x_smote[len(x_train):]\n",
    "synthetic_adasyn = x_adasyn[len(x_train):]\n",
    "\n",
    "print(f\"Number of synthetic samples - SMOTE: {len(synthetic_smote)}, ADASYN: {len(synthetic_adasyn)}\") \n",
    "\n",
    "# Only proceed if we have synthetic samples\n",
    "if len(synthetic_smote) > 0 and len(synthetic_adasyn) > 0:\n",
    "    # Merge synthetic samples\n",
    "    synthetic_samples_list = [synthetic_smote, synthetic_adasyn]\n",
    "    #weights = [0.45, 0.55]  # Giving slightly more weight to SMOTE\n",
    "    weights = [0.35, 0.65]\n",
    "    merged_synthetic = merge_synthetic_samples(synthetic_samples_list, weights)\n",
    "\n",
    "    # Combine original samples with merged synthetic samples\n",
    "    x_merged = np.vstack([x_train, merged_synthetic])\n",
    "    y_merged = np.concatenate([y_train, np.ones(len(merged_synthetic))])\n",
    "    merged_model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.25)\n",
    "    merged_model.fit(x_merged, y_merged)\n",
    "    ensemble_model.fit(x_merged, y_merged)\n",
    "    joblib.dump(merged_model, 'merged_model.pkl')\n",
    "    # Evaluate the merged approach\n",
    "    merged_accuracy, merged_f1, merged_recall, merged_precision = evaluate_model(x_merged, y_merged, x_test, y_test, \n",
    "                                   XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.25))\n",
    "\n",
    "    # Print individual method accuracies for comparison\n",
    "    smote_accuracy, smote_f1, smote_recall, smote_precision = evaluate_model(x_smote, y_smote, x_test, y_test, \n",
    "                                  XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "    adasyn_accuracy, adasyn_f1, adasyn_recall, adasyn_precision = evaluate_model(x_adasyn, y_adasyn, x_test, y_test, \n",
    "                                   XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "    ensemble_accuracy, ensemble_f1, ensemble_recall, ensemble_precision = evaluate_model(x_merged, y_merged, x_test, y_test, \n",
    "                                     VotingClassifier(\n",
    "    estimators=[('smote', smote_xg), ('adasyn', adasyn_xg), ('merged', merged_model), ('smotenn', smote_tn_model)], voting='soft'\n",
    "))\n",
    "\n",
    "    print(f'SMOTE Accuracy: {smote_accuracy * 100:.2f}%')\n",
    "    print(f'SMOTE f1: {smote_f1:.2f}')\n",
    "    print(f'SMOTE Recall: {smote_recall:.2f}')\n",
    "    print(f'SMOTE Precision: {smote_precision:.2f}')\n",
    "    print(\"\\n\\n\")\n",
    "    print(f'ADASYN Accuracy: {adasyn_accuracy * 100:.2f}%')\n",
    "    print(f'ADASYN f1: {adasyn_f1:.2f}')\n",
    "    print(f'ADASYN Recall: {adasyn_recall:.2f}')\n",
    "    print(f'ADASYN Precision: {adasyn_precision:.2f}')\n",
    "    print(\"\\n\\n\")\n",
    "    print(f'Merged Approach Accuracy: {merged_accuracy * 100:.2f}%')\n",
    "    print(f'Merged Approach f1: {merged_f1:.2f}')\n",
    "    print(f'Merged Approach Recall: {merged_recall:.2f}')\n",
    "    print(f'Merged Approach Precision: {merged_precision:.2f}')\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Ensemble approach accuracy: {ensemble_accuracy * 100:.2f}\")\n",
    "    print(f\"Enseble F1: {ensemble_f1}\")\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Error: One or more synthetic sample sets are empty\")\n",
    "    print(\"Please ensure the minority class has samples to generate synthetic data\")\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train base models on different datasets\n",
    "model_smote = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3)\n",
    "model_adasyn = RandomForestClassifier(n_estimators=100)\n",
    "model_original = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3)\n",
    "\n",
    "# Create the Voting Classifier (hard voting for majority class, soft voting for probabilities)\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('smote', model_smote), \n",
    "    ('adasyn', model_adasyn), \n",
    "    ('original', model_original)\n",
    "], voting='hard')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of different augmentation methods...\n",
      "\n",
      "Evaluating SMOTE...\n",
      "\n",
      "Evaluating ADASYN...\n",
      "\n",
      "Evaluating combined SMOTE-ADASYN approach...\n",
      "Starting data augmentation pipeline...\n",
      "Initial dataset size: 1601\n",
      "Initial class distribution - Positive: 422, Negative: 1179\n",
      "\n",
      "Phase 1: ADASYN augmentation\n",
      "\n",
      "Phase 2: SMOTE augmentation\n",
      "\n",
      "Augmentation complete!\n",
      "Final dataset size: 1650600\n",
      "Final class distribution - Positive: 825300, Negative: 825300\n",
      "\n",
      "Results:\n",
      "Baseline Accuracy: 85.62%\n",
      "Smote Accuracy: 86.50%\n",
      "Adasyn Accuracy: 86.38%\n",
      "Combined Accuracy: 86.88%\n",
      "\n",
      "Best performing method: combined\n",
      "Best accuracy: 86.88%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def augment_dataset(x_train, y_train, target_size_multiplier=700):\n",
    "    \"\"\"\n",
    "    Augment the dataset using a combination of SMOTE and ADASYN in phases\n",
    "    to achieve a balanced dataset that is target_size_multiplier times larger\n",
    "    \"\"\"\n",
    "    print(\"Starting data augmentation pipeline...\")\n",
    "    \n",
    "    # Get initial class distribution\n",
    "    initial_size = len(y_train)\n",
    "    pos_samples = np.sum(y_train == 1)\n",
    "    neg_samples = np.sum(y_train == 0)\n",
    "    target_samples = max(pos_samples, neg_samples) * target_size_multiplier\n",
    "    \n",
    "    print(f\"Initial dataset size: {initial_size}\")\n",
    "    print(f\"Initial class distribution - Positive: {pos_samples}, Negative: {neg_samples}\")\n",
    "    \n",
    "    # Phase 1: Use ADASYN for initial positive class augmentation\n",
    "    print(\"\\nPhase 1: ADASYN augmentation\")\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # Phase 2: Use SMOTE to further augment the dataset\n",
    "    print(\"\\nPhase 2: SMOTE augmentation\")\n",
    "    sampling_strategy = {\n",
    "        0: target_samples,\n",
    "        1: target_samples\n",
    "    }\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_final, y_final = smote.fit_resample(X_adasyn, y_adasyn)\n",
    "    \n",
    "    # Print final statistics\n",
    "    final_pos = np.sum(y_final == 1)\n",
    "    final_neg = np.sum(y_final == 0)\n",
    "    print(\"\\nAugmentation complete!\")\n",
    "    print(f\"Final dataset size: {len(y_final)}\")\n",
    "    print(f\"Final class distribution - Positive: {final_pos}, Negative: {final_neg}\")\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "def evaluate_augmentation_methods(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate different augmentation methods and their combinations\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Base XGBoost configuration\n",
    "    base_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 3,\n",
    "        'learning_rate': 0.5\n",
    "    }\n",
    "    \n",
    "    # Original data (baseline)\n",
    "    models['baseline'] = XGBClassifier(**base_params)\n",
    "    models['baseline'].fit(x_train, y_train)\n",
    "    results['baseline'] = accuracy_score(y_test, models['baseline'].predict(x_test))\n",
    "    \n",
    "    # SMOTE\n",
    "    print(\"\\nEvaluating SMOTE...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "    models['smote'] = XGBClassifier(**base_params)\n",
    "    models['smote'].fit(X_smote, y_smote)\n",
    "    results['smote'] = accuracy_score(y_test, models['smote'].predict(x_test))\n",
    "    \n",
    "    # ADASYN\n",
    "    print(\"\\nEvaluating ADASYN...\")\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "    models['adasyn'] = XGBClassifier(**base_params)\n",
    "    models['adasyn'].fit(X_adasyn, y_adasyn)\n",
    "    results['adasyn'] = accuracy_score(y_test, models['adasyn'].predict(x_test))\n",
    "    \n",
    "    # Combined approach\n",
    "    print(\"\\nEvaluating combined SMOTE-ADASYN approach...\")\n",
    "    X_combined, y_combined = augment_dataset(x_train, y_train)\n",
    "    models['combined'] = XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.04)\n",
    "    models['combined'].fit(X_combined, y_combined)\n",
    "    results['combined'] = accuracy_score(y_test, models['combined'].predict(x_test))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    for method, acc in results.items():\n",
    "        print(f\"{method.capitalize()} Accuracy: {acc * 100:.2f}%\")\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "# Execute the augmentation and evaluation\n",
    "print(\"Starting evaluation of different augmentation methods...\")\n",
    "models, results = evaluate_augmentation_methods(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Use the best performing model for final predictions\n",
    "best_method = max(results.items(), key=lambda x: x[1])[0]\n",
    "best_model = models[best_method]\n",
    "final_predictions = best_model.predict(x_test)\n",
    "\n",
    "print(f\"\\nBest performing method: {best_method}\")\n",
    "print(f\"Best accuracy: {results[best_method] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
