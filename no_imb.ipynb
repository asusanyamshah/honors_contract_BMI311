{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2401, 13)\n",
      "1579\n",
      "   Age     Sex  Income   Race  WaistCirc   BMI  Albuminuria  UrAlbCr  \\\n",
      "0   22    Male  8200.0  White       81.0  23.3            0     3.88   \n",
      "1   44  Female  4500.0  White       80.1  23.2            0     8.55   \n",
      "2   21    Male   800.0  Asian       69.6  20.1            0     5.07   \n",
      "3   43  Female  2000.0  Black      120.4  33.3            0     5.22   \n",
      "4   51    Male     NaN  Asian       81.1  20.1            0     8.13   \n",
      "\n",
      "   UricAcid  BloodGlucose  HDL  Triglycerides  MetabolicSyndrome  \n",
      "0       4.9            92   41             84                  0  \n",
      "1       4.5            82   28             56                  0  \n",
      "2       5.4           107   43             78                  0  \n",
      "3       5.0           104   73            141                  0  \n",
      "4       5.0            95   43            126                  0  \n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "\n",
    "# Defining the path for the csv file\n",
    "path = os.path.join(\"dataset.csv\")\n",
    "\n",
    "# Storing the dataframe in a variable named dataset\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "dataset = dataset.drop('seqn', axis='columns')\n",
    "dataset = dataset.drop('Marital', axis='columns')\n",
    "print(dataset.shape)\n",
    "print(len(dataset[dataset['MetabolicSyndrome'] == 0]))\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age     Sex    Income   Race  WaistCirc       BMI  Albuminuria  \\\n",
      "0  0.033333    Male  0.908046  White   0.207012  0.179024          0.0   \n",
      "1  0.400000  Female  0.482759  White   0.199499  0.177215          0.0   \n",
      "2  0.016667    Male  0.057471  Asian   0.111853  0.121157          0.0   \n",
      "3  0.383333  Female  0.195402  Black   0.535893  0.359855          0.0   \n",
      "4  0.516667    Male       NaN  Asian   0.207846  0.121157          0.0   \n",
      "\n",
      "    UrAlbCr  UricAcid  BloodGlucose       HDL  Triglycerides  \\\n",
      "0  0.000418  0.326316      0.154519  0.190141       0.037760   \n",
      "1  0.001206  0.284211      0.125364  0.098592       0.019531   \n",
      "2  0.000619  0.378947      0.198251  0.204225       0.033854   \n",
      "3  0.000645  0.336842      0.189504  0.415493       0.074870   \n",
      "4  0.001136  0.336842      0.163265  0.204225       0.065104   \n",
      "\n",
      "   MetabolicSyndrome  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Defining the columns that need scaling\n",
    "columns_to_scale = ['Age', 'Income', 'WaistCirc', 'BMI', 'Albuminuria', \n",
    "                    'UrAlbCr', 'UricAcid', 'BloodGlucose', 'HDL', 'Triglycerides']\n",
    "\n",
    "# Creating a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scaling the selected columns\n",
    "dataset[columns_to_scale] = scaler.fit_transform(dataset[columns_to_scale])\n",
    "\n",
    "# Check the transformed dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age  Sex    Income  Race  WaistCirc       BMI  Albuminuria   UrAlbCr  \\\n",
      "0  0.033333    0  0.908046     0   0.207012  0.179024          0.0  0.000418   \n",
      "1  0.400000    1  0.482759     0   0.199499  0.177215          0.0  0.001206   \n",
      "2  0.016667    0  0.057471     1   0.111853  0.121157          0.0  0.000619   \n",
      "3  0.383333    1  0.195402     2   0.535893  0.359855          0.0  0.000645   \n",
      "4  0.516667    0  0.425891     1   0.207846  0.121157          0.0  0.001136   \n",
      "\n",
      "   UricAcid  BloodGlucose       HDL  Triglycerides  MetabolicSyndrome  \n",
      "0  0.326316      0.154519  0.190141       0.037760                  0  \n",
      "1  0.284211      0.125364  0.098592       0.019531                  0  \n",
      "2  0.378947      0.198251  0.204225       0.033854                  0  \n",
      "3  0.336842      0.189504  0.415493       0.074870                  0  \n",
      "4  0.336842      0.163265  0.204225       0.065104                  0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/p012qq114lj2w2v6rb72_p1c0000gn/T/ipykernel_35480/4086439163.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
      "/var/folders/8t/p012qq114lj2w2v6rb72_p1c0000gn/T/ipykernel_35480/4086439163.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Race'] = dataset['Race'].replace(race_mapping)\n"
     ]
    }
   ],
   "source": [
    "sex_mapping = {'Male': 0, 'Female': 1}\n",
    "race_mapping = {'White': 0, 'Asian': 1, 'Black': 2, 'MexAmerican': 3, 'Hispanic': 4, 'Other': 5}\n",
    "\n",
    "dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
    "dataset['Race'] = dataset['Race'].replace(race_mapping)\n",
    "\n",
    "# This is the incorrect implementation\n",
    "'''\n",
    "dataset = dataset.fillna(2)\n",
    "dataset = dataset.fillna(4)\n",
    "dataset = dataset.fillna(5)\n",
    "'''\n",
    "# Fill NaN values in column with index 2\n",
    "dataset.iloc[:, 2] = dataset.iloc[:, 2].fillna(dataset.iloc[:, 2].mean())\n",
    "\n",
    "# Fill NaN values in column with index 4\n",
    "dataset.iloc[:, 4] = dataset.iloc[:, 4].fillna(dataset.iloc[:, 4].mean())\n",
    "\n",
    "# Fill NaN values in column with index 5\n",
    "dataset.iloc[:, 5] = dataset.iloc[:, 5].fillna(dataset.iloc[:, 5].mean())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_0 = dataset[dataset['MetabolicSyndrome'] == 0]\n",
    "outcome_1 = dataset[dataset['MetabolicSyndrome'] == 1]\n",
    "\n",
    "\n",
    "test_size_each_class = 400\n",
    "test_0 = outcome_0.sample(n=test_size_each_class, random_state=42)\n",
    "test_1 = outcome_1.sample(n=test_size_each_class, random_state=42)\n",
    "#print(test_1)\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Remove the test set rows from the original dataset to create the training set\n",
    "train_data = dataset.drop(test_data.index)\n",
    "\n",
    "\n",
    "x_train = train_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_train = train_data['MetabolicSyndrome'].values\n",
    "x_test = test_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_test = test_data['MetabolicSyndrome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_xgboost = XGBClassifier(n_estimators=100, learning_rate=0.3, max_depth=3, random_state=42)\n",
    "classifier_xgboost.fit(x_train, y_train)\n",
    "y_pred_xgboost = classifier_xgboost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8625\n",
      "F1 Score: 0.8497267759562842\n",
      "Recall: 0.7775\n",
      "Precision: 0.9367469879518072\n",
      "[[379  21]\n",
      " [ 89 311]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_xgboost)\n",
    "f1 = f1_score(y_test, y_pred_xgboost)\n",
    "recall = recall_score(y_test, y_pred_xgboost)\n",
    "precision = precision_score(y_test, y_pred_xgboost)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(confusion_matrix(y_test, y_pred_xgboost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
