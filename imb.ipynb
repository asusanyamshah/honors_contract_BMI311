{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "import os\n",
    "\n",
    "# Defining the path for the csv file\n",
    "path = os.path.join(\"dataset.csv\")\n",
    "\n",
    "# Storing the dataframe in a variable named dataset\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "dataset = dataset.drop('seqn', axis='columns')\n",
    "dataset = dataset.drop('Marital', axis='columns')\n",
    "print(dataset.shape)\n",
    "print(len(dataset[dataset['MetabolicSyndrome'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorical variables and filling in the missing values\n",
    "sex_mapping = {'Male': 0, 'Female': 1}\n",
    "race_mapping = {'White': 0, 'Asian': 1, 'Black': 2, 'MexAmerican': 3, 'Hispanic': 4, 'Other': 5}\n",
    "\n",
    "dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
    "dataset['Race'] = dataset['Race'].replace(race_mapping)\n",
    "\n",
    "# This is the incorrect implementation\n",
    "'''\n",
    "dataset = dataset.fillna(2)\n",
    "dataset = dataset.fillna(4)\n",
    "dataset = dataset.fillna(5)\n",
    "'''\n",
    "# Fill NaN values in column with index 2\n",
    "dataset.iloc[:, 2] = dataset.iloc[:, 2].fillna(dataset.iloc[:, 2].mean())\n",
    "\n",
    "# Fill NaN values in column with index 4\n",
    "dataset.iloc[:, 4] = dataset.iloc[:, 4].fillna(dataset.iloc[:, 4].mean())\n",
    "\n",
    "# Fill NaN values in column with index 5\n",
    "dataset.iloc[:, 5] = dataset.iloc[:, 5].fillna(dataset.iloc[:, 5].mean())\n",
    "\n",
    "outcome_0 = dataset[dataset['MetabolicSyndrome'] == 0]\n",
    "outcome_1 = dataset[dataset['MetabolicSyndrome'] == 1]\n",
    "\n",
    "\n",
    "test_size_each_class = 400\n",
    "test_0 = outcome_0.sample(n=test_size_each_class, random_state=42)\n",
    "test_1 = outcome_1.sample(n=test_size_each_class, random_state=42)\n",
    "#print(test_1)\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Remove the test set rows from the original dataset to create the training set\n",
    "train_data = dataset.drop(test_data.index)\n",
    "\n",
    "\n",
    "x_train = train_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_train = train_data['MetabolicSyndrome'].values\n",
    "x_test = test_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_test = test_data['MetabolicSyndrome'].values\n",
    "\n",
    "# Resampling the data to avoid overfitting\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "# Resampling the data\n",
    "x_resampled, y_resampled = ros.fit_resample(x_train, y_train)\n",
    "print(\"x_train dtypes:\", x_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "\n",
    "# XGBoost Classifier\n",
    "classifier_xgboost = XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.5)\n",
    "classifier_xgboost = XGBClassifier(n_estimators = 412, max_depth = 8, learning_rate = 0.5)\n",
    "classifier_xgboost.fit(x_resampled, y_resampled)\n",
    "# joblib.dump(classifier_xgboost, 'xgboost_classifier.pkl')\n",
    "y_pred = classifier_xgboost.predict(x_test)\n",
    "#for i in x_test[0]:\n",
    "   # print(type(i))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accracy: \")\n",
    "print(f'\\t{(accuracy_score(y_test, y_pred) * 100):.2f}% is the accuracy\\n')\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from ctgan import CTGAN\n",
    "\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(x_train, y_train, x_test, y_test, model):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "smote_accuracy = evaluate_model(x_smote, y_smote, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "\n",
    "smote_t = SMOTETomek(random_state=0)\n",
    "x_smote_t, y_smote_t = smote_t.fit_resample(x_train, y_train)\n",
    "smote_t_accuracy = evaluate_model(x_smote_t, y_smote_t, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "\n",
    "smote_tn = SMOTEENN(random_state=0)\n",
    "x_smote_tn, y_smote_tn = smote_tn.fit_resample(x_train, y_train)\n",
    "smote_tn_accuracy = evaluate_model(x_smote_tn, y_smote_tn, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "\n",
    "\n",
    "# ADASYN\n",
    "adasyn = ADASYN(random_state=0)\n",
    "x_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "adasyn_accuracy = evaluate_model(x_adasyn, y_adasyn, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "\n",
    "# CTGAN\n",
    "ctgan = CTGAN(epochs=10)\n",
    "ctgan.fit(x_train, y_train)\n",
    "x_ctgan = ctgan.sample(len(x_train))\n",
    "y_ctgan = y_train  # Optionally reuse y_train if it makes sense for your scenario\n",
    "ctgan_accuracy = evaluate_model(x_ctgan, y_ctgan, x_test, y_test, XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.5))\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'SMOTE Accuracy: {smote_accuracy * 100:.2f}%')\n",
    "print(f'SMOTETomek Accuracy: {smote_t_accuracy * 100:.2f}%')\n",
    "print(f'SMOTEENN Accuracy: {smote_tn_accuracy * 100:.2f}%')\n",
    "print(f'ADASYN Accuracy: {adasyn_accuracy * 100:.2f}%')\n",
    "print(f'CTGAN Accuracy: {ctgan_accuracy * 100:.2f}%')\n",
    "'''\n",
    "print(\"Confusion matrix: \")\n",
    "print(cm, \"\\n\")\n",
    "print(\"Precision Score: \")\n",
    "print(\"\\t\",precision_score(y_test, y_pred), \"\\n\")\n",
    "print(\"Recall: \")\n",
    "print(\"\\t\", recall_score(y_test, y_pred), \"\\n\")\n",
    "print(\"F1 Score: \")\n",
    "print(\"\\t\", f1_score(y_test, y_pred), \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def augment_dataset(x_train, y_train, target_size_multiplier=5):\n",
    "    \"\"\"\n",
    "    Augment the dataset using a combination of SMOTE and ADASYN in phases\n",
    "    to achieve a balanced dataset that is target_size_multiplier times larger\n",
    "    \"\"\"\n",
    "    print(\"Starting data augmentation pipeline...\")\n",
    "    \n",
    "    # Get initial class distribution\n",
    "    initial_size = len(y_train)\n",
    "    pos_samples = np.sum(y_train == 1)\n",
    "    neg_samples = np.sum(y_train == 0)\n",
    "    target_samples = max(pos_samples, neg_samples) * target_size_multiplier\n",
    "    \n",
    "    print(f\"Initial dataset size: {initial_size}\")\n",
    "    print(f\"Initial class distribution - Positive: {pos_samples}, Negative: {neg_samples}\")\n",
    "    \n",
    "    # Phase 1: Use ADASYN for initial positive class augmentation\n",
    "    print(\"\\nPhase 1: ADASYN augmentation\")\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # Phase 2: Use SMOTE to further augment the dataset\n",
    "    print(\"\\nPhase 2: SMOTE augmentation\")\n",
    "    sampling_strategy = {\n",
    "        0: target_samples,\n",
    "        1: target_samples\n",
    "    }\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_final, y_final = smote.fit_resample(X_adasyn, y_adasyn)\n",
    "    \n",
    "    # Print final statistics\n",
    "    final_pos = np.sum(y_final == 1)\n",
    "    final_neg = np.sum(y_final == 0)\n",
    "    print(\"\\nAugmentation complete!\")\n",
    "    print(f\"Final dataset size: {len(y_final)}\")\n",
    "    print(f\"Final class distribution - Positive: {final_pos}, Negative: {final_neg}\")\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "def evaluate_augmentation_methods(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate different augmentation methods and their combinations\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Base XGBoost configuration\n",
    "    base_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 3,\n",
    "        'learning_rate': 0.5\n",
    "    }\n",
    "    \n",
    "    # Original data (baseline)\n",
    "    models['baseline'] = XGBClassifier(**base_params)\n",
    "    models['baseline'].fit(x_train, y_train)\n",
    "    results['baseline'] = accuracy_score(y_test, models['baseline'].predict(x_test))\n",
    "    \n",
    "    # SMOTE\n",
    "    print(\"\\nEvaluating SMOTE...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "    models['smote'] = XGBClassifier(**base_params)\n",
    "    models['smote'].fit(X_smote, y_smote)\n",
    "    results['smote'] = accuracy_score(y_test, models['smote'].predict(x_test))\n",
    "    \n",
    "    # ADASYN\n",
    "    print(\"\\nEvaluating ADASYN...\")\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "    models['adasyn'] = XGBClassifier(**base_params)\n",
    "    models['adasyn'].fit(X_adasyn, y_adasyn)\n",
    "    results['adasyn'] = accuracy_score(y_test, models['adasyn'].predict(x_test))\n",
    "    \n",
    "    # Combined approach\n",
    "    print(\"\\nEvaluating combined SMOTE-ADASYN approach...\")\n",
    "    X_combined, y_combined = augment_dataset(x_train, y_train)\n",
    "    models['combined'] = XGBClassifier(**base_params)\n",
    "    models['combined'].fit(X_combined, y_combined)\n",
    "    results['combined'] = accuracy_score(y_test, models['combined'].predict(x_test))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    for method, acc in results.items():\n",
    "        print(f\"{method.capitalize()} Accuracy: {acc * 100:.2f}%\")\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "# Execute the augmentation and evaluation\n",
    "print(\"Starting evaluation of different augmentation methods...\")\n",
    "models, results = evaluate_augmentation_methods(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Use the best performing model for final predictions\n",
    "best_method = max(results.items(), key=lambda x: x[1])[0]\n",
    "best_model = models[best_method]\n",
    "final_predictions = best_model.predict(x_test)\n",
    "\n",
    "print(f\"\\nBest performing method: {best_method}\")\n",
    "print(f\"Best accuracy: {results[best_method] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def merge_synthetic_samples(synthetic_samples_list, weights=None):\n",
    "    \"\"\"\n",
    "    Merge multiple sets of synthetic samples using weighted nearest neighbor averaging.\n",
    "    \n",
    "    Parameters:\n",
    "    synthetic_samples_list: List of numpy arrays, each containing synthetic samples from different methods\n",
    "    weights: List of weights for each method (default: equal weights)\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: Merged synthetic samples\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1/len(synthetic_samples_list)] * len(synthetic_samples_list)\n",
    "    \n",
    "    # Ensure we have valid samples\n",
    "    for i, samples in enumerate(synthetic_samples_list):\n",
    "        if len(samples) == 0:\n",
    "            raise ValueError(f\"Synthetic samples list at index {i} is empty\")\n",
    "    \n",
    "    # Ensure all sample sets have the same number of features\n",
    "    n_features = synthetic_samples_list[0].shape[1]\n",
    "    \n",
    "    # Initialize nearest neighbor models for each synthetic dataset\n",
    "    nn_models = []\n",
    "    for samples in synthetic_samples_list:\n",
    "        nn = NearestNeighbors(n_neighbors=1)\n",
    "        nn.fit(samples)\n",
    "        nn_models.append(nn)\n",
    "    \n",
    "    # Use the first synthetic dataset as a reference\n",
    "    base_samples = synthetic_samples_list[0]\n",
    "    merged_samples = np.zeros_like(base_samples)\n",
    "    \n",
    "    # For each sample in the base dataset\n",
    "    for i in range(len(base_samples)):\n",
    "        sample_sum = np.zeros(n_features)\n",
    "        \n",
    "        # Find nearest neighbors in each synthetic dataset\n",
    "        for j, (samples, nn_model, weight) in enumerate(zip(synthetic_samples_list, nn_models, weights)):\n",
    "            if j == 0:\n",
    "                # For the base dataset, use the sample itself\n",
    "                nearest_sample = base_samples[i]\n",
    "            else:\n",
    "                # Find nearest neighbor in other datasets\n",
    "                distances, indices = nn_model.kneighbors([base_samples[i]])\n",
    "                nearest_sample = samples[indices[0][0]]\n",
    "            \n",
    "            sample_sum += weight * nearest_sample\n",
    "        \n",
    "        merged_samples[i] = sample_sum\n",
    "    \n",
    "    return merged_samples\n",
    "\n",
    "# First, let's analyze the class distribution\n",
    "n_minority = np.sum(y_train == 1)\n",
    "n_majority = np.sum(y_train == 0)\n",
    "print(f\"Original distribution - Majority: {n_majority}, Minority: {n_minority}\")\n",
    "\n",
    "# Generate synthetic samples with SMOTE and ADASYN\n",
    "smote = SMOTE(random_state=42)\n",
    "adasyn = ADASYN(random_state=42)\n",
    "ctgan = CTGAN(epochs=500)\n",
    "ctgan.fit(x_train, y_train)\n",
    "x_ctgan = ctgan.sample(len(x_train))\n",
    "y_ctgan = y_train  \n",
    "\n",
    "# Generate synthetic samples\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "x_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)\n",
    "\n",
    "# Extract only the synthetic samples (exclude original samples)\n",
    "synthetic_smote = x_smote[len(x_train):]\n",
    "synthetic_adasyn = x_adasyn[len(x_train):]\n",
    "synthetic_ctgan = x_ctgan[len(x_train):]\n",
    "\n",
    "print(f\"Number of synthetic samples - SMOTE: {len(synthetic_smote)}, ADASYN: {len(synthetic_adasyn)}, CTGAN: {len(synthetic_ctgan)}\")\n",
    "\n",
    "# Only proceed if we have synthetic samples\n",
    "if len(synthetic_smote) > 0 and len(synthetic_adasyn) > 0:\n",
    "    # Merge synthetic samples\n",
    "    synthetic_samples_list = [synthetic_smote, synthetic_adasyn]\n",
    "    weights = [0.8, 0.2]  # Giving slightly more weight to SMOTE\n",
    "    merged_synthetic = merge_synthetic_samples(synthetic_samples_list, weights)\n",
    "\n",
    "    # Combine original samples with merged synthetic samples\n",
    "    x_merged = np.vstack([x_train, merged_synthetic])\n",
    "    y_merged = np.concatenate([y_train, np.ones(len(merged_synthetic))])\n",
    "\n",
    "    # Evaluate the merged approach\n",
    "    merged_accuracy = evaluate_model(x_merged, y_merged, x_test, y_test, \n",
    "                                   XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "\n",
    "    # Print individual method accuracies for comparison\n",
    "    smote_accuracy = evaluate_model(x_smote, y_smote, x_test, y_test, \n",
    "                                  XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "    adasyn_accuracy = evaluate_model(x_adasyn, y_adasyn, x_test, y_test, \n",
    "                                   XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.3))\n",
    "\n",
    "    print(f'SMOTE Accuracy: {smote_accuracy * 100:.2f}%')\n",
    "    print(f'ADASYN Accuracy: {adasyn_accuracy * 100:.2f}%')\n",
    "    print(f'Merged Approach Accuracy: {merged_accuracy * 100:.2f}%')\n",
    "else:\n",
    "    print(\"Error: One or more synthetic sample sets are empty\")\n",
    "    print(\"Please ensure the minority class has samples to generate synthetic data\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
