{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2401, 13)\n",
      "1579\n",
      "   Age     Sex  Income   Race  WaistCirc   BMI  Albuminuria  UrAlbCr  \\\n",
      "0   22    Male  8200.0  White       81.0  23.3            0     3.88   \n",
      "1   44  Female  4500.0  White       80.1  23.2            0     8.55   \n",
      "2   21    Male   800.0  Asian       69.6  20.1            0     5.07   \n",
      "3   43  Female  2000.0  Black      120.4  33.3            0     5.22   \n",
      "4   51    Male     NaN  Asian       81.1  20.1            0     8.13   \n",
      "\n",
      "   UricAcid  BloodGlucose  HDL  Triglycerides  MetabolicSyndrome  \n",
      "0       4.9            92   41             84                  0  \n",
      "1       4.5            82   28             56                  0  \n",
      "2       5.4           107   43             78                  0  \n",
      "3       5.0           104   73            141                  0  \n",
      "4       5.0            95   43            126                  0  \n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "\n",
    "# Defining the path for the csv file\n",
    "path = os.path.join(\"dataset.csv\")\n",
    "\n",
    "# Storing the dataframe in a variable named dataset\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "dataset = dataset.drop('seqn', axis='columns')\n",
    "dataset = dataset.drop('Marital', axis='columns')\n",
    "print(dataset.shape)\n",
    "print(len(dataset[dataset['MetabolicSyndrome'] == 0]))\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age     Sex    Income   Race  WaistCirc       BMI  Albuminuria  \\\n",
      "0  0.033333    Male  0.908046  White   0.207012  0.179024          0.0   \n",
      "1  0.400000  Female  0.482759  White   0.199499  0.177215          0.0   \n",
      "2  0.016667    Male  0.057471  Asian   0.111853  0.121157          0.0   \n",
      "3  0.383333  Female  0.195402  Black   0.535893  0.359855          0.0   \n",
      "4  0.516667    Male       NaN  Asian   0.207846  0.121157          0.0   \n",
      "\n",
      "    UrAlbCr  UricAcid  BloodGlucose       HDL  Triglycerides  \\\n",
      "0  0.000418  0.326316      0.154519  0.190141       0.037760   \n",
      "1  0.001206  0.284211      0.125364  0.098592       0.019531   \n",
      "2  0.000619  0.378947      0.198251  0.204225       0.033854   \n",
      "3  0.000645  0.336842      0.189504  0.415493       0.074870   \n",
      "4  0.001136  0.336842      0.163265  0.204225       0.065104   \n",
      "\n",
      "   MetabolicSyndrome  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Defining the columns that need scaling\n",
    "columns_to_scale = ['Age', 'Income', 'WaistCirc', 'BMI', 'Albuminuria', \n",
    "                    'UrAlbCr', 'UricAcid', 'BloodGlucose', 'HDL', 'Triglycerides']\n",
    "\n",
    "# Creating a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scaling the selected columns\n",
    "dataset[columns_to_scale] = scaler.fit_transform(dataset[columns_to_scale])\n",
    "\n",
    "# Check the transformed dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age  Sex    Income  Race  WaistCirc       BMI  Albuminuria   UrAlbCr  \\\n",
      "0  0.033333    0  0.908046     0   0.207012  0.179024          0.0  0.000418   \n",
      "1  0.400000    1  0.482759     0   0.199499  0.177215          0.0  0.001206   \n",
      "2  0.016667    0  0.057471     1   0.111853  0.121157          0.0  0.000619   \n",
      "3  0.383333    1  0.195402     2   0.535893  0.359855          0.0  0.000645   \n",
      "4  0.516667    0  0.425891     1   0.207846  0.121157          0.0  0.001136   \n",
      "\n",
      "   UricAcid  BloodGlucose       HDL  Triglycerides  MetabolicSyndrome  \n",
      "0  0.326316      0.154519  0.190141       0.037760                  0  \n",
      "1  0.284211      0.125364  0.098592       0.019531                  0  \n",
      "2  0.378947      0.198251  0.204225       0.033854                  0  \n",
      "3  0.336842      0.189504  0.415493       0.074870                  0  \n",
      "4  0.336842      0.163265  0.204225       0.065104                  0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/p012qq114lj2w2v6rb72_p1c0000gn/T/ipykernel_36326/4086439163.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
      "/var/folders/8t/p012qq114lj2w2v6rb72_p1c0000gn/T/ipykernel_36326/4086439163.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Race'] = dataset['Race'].replace(race_mapping)\n"
     ]
    }
   ],
   "source": [
    "sex_mapping = {'Male': 0, 'Female': 1}\n",
    "race_mapping = {'White': 0, 'Asian': 1, 'Black': 2, 'MexAmerican': 3, 'Hispanic': 4, 'Other': 5}\n",
    "\n",
    "dataset['Sex'] = dataset['Sex'].replace(sex_mapping)\n",
    "dataset['Race'] = dataset['Race'].replace(race_mapping)\n",
    "\n",
    "# This is the incorrect implementation\n",
    "'''\n",
    "dataset = dataset.fillna(2)\n",
    "dataset = dataset.fillna(4)\n",
    "dataset = dataset.fillna(5)\n",
    "'''\n",
    "# Fill NaN values in column with index 2\n",
    "dataset.iloc[:, 2] = dataset.iloc[:, 2].fillna(dataset.iloc[:, 2].mean())\n",
    "\n",
    "# Fill NaN values in column with index 4\n",
    "dataset.iloc[:, 4] = dataset.iloc[:, 4].fillna(dataset.iloc[:, 4].mean())\n",
    "\n",
    "# Fill NaN values in column with index 5\n",
    "dataset.iloc[:, 5] = dataset.iloc[:, 5].fillna(dataset.iloc[:, 5].mean())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_0 = dataset[dataset['MetabolicSyndrome'] == 0]\n",
    "outcome_1 = dataset[dataset['MetabolicSyndrome'] == 1]\n",
    "test_size_each_class = 400\n",
    "\n",
    "# Create test sets\n",
    "test_0 = outcome_0.sample(n=test_size_each_class, random_state=42)\n",
    "test_1 = outcome_1.sample(n=test_size_each_class, random_state=42)\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Create training set\n",
    "train_data = dataset.drop(test_data.index)\n",
    "\n",
    "# Get the counts of each class in training data\n",
    "train_0 = train_data[train_data['MetabolicSyndrome'] == 0]\n",
    "train_1 = train_data[train_data['MetabolicSyndrome'] == 1]\n",
    "\n",
    "# Determine how many synthetic samples we need\n",
    "# Generate samples for the minority class to match the majority class\n",
    "if len(train_0) > len(train_1):\n",
    "    samples_needed = len(train_0) - len(train_1)\n",
    "    majority_class = len(train_0)\n",
    "else:\n",
    "    samples_needed = len(train_1) - len(train_0)\n",
    "    majority_class = len(train_1)\n",
    "\n",
    "# Using CTGAN to balance the classes\n",
    "from ctgan import CTGAN\n",
    "ctgan = CTGAN(epochs=500)\n",
    "ctgan.fit(train_data, discrete_columns=['Sex', 'Race', 'MetabolicSyndrome'])\n",
    "\n",
    "# Generate synthetic samples (ensure positive number)\n",
    "synthetic_data = ctgan.sample(samples_needed)\n",
    "\n",
    "# Combine synthetic data with the original training data\n",
    "train_data_balanced = pd.concat([train_data, synthetic_data])\n",
    "\n",
    "# Separate features and target variable from the balanced dataset\n",
    "x_train_balanced = train_data_balanced.drop('MetabolicSyndrome', axis=1).values\n",
    "y_train_balanced = train_data_balanced['MetabolicSyndrome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.840\n",
      "Precision: 0.907\n",
      "Recall: 0.757\n",
      "F1 Score: 0.826\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       400\n",
      "           1       0.91      0.76      0.83       400\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.85      0.84      0.84       800\n",
      "weighted avg       0.85      0.84      0.84       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test features and target from test_data\n",
    "x_test = test_data.drop('MetabolicSyndrome', axis=1).values\n",
    "y_test = test_data['MetabolicSyndrome'].values\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
